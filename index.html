<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yanda Chen</title>
  
  <meta name="author" content="Yanda Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yanda Chen</name>
              </p>
              <p>I am a second-year PhD student in Computer Science at Columbia University working on natural language processing and machine learning. I am very fortunate to be co-advised by <a href="http://www.cs.columbia.edu/~kathy/">Prof. Kathy McKeown</a>, <a href="https://hhexiy.github.io/">Prof. He He</a> (NYU), and <a href="http://www.cs.columbia.edu/~zhouyu/">Prof. Zhou Yu</a>. Previously, I received my bachelor's degree in Computer Science at Columbia University in April 
		      
		      21. 
              </p>
              <p style="text-align:center">
                <a href="mailto:yanda.chen@cs.columbia.edu">Email</a> &nbsp/&nbsp
                <a href="data/Yanda Chen CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Yanda-Chen/2109268730">Semantic Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/yanda_chen_">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/yandachen">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/photo.jpeg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/photo.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p> My current research interest lies in three directions: i) building deep learning systems in low-resource settings (zero/few-shot learning, domain adaptation, meta-learning, etc.), ii) improving system efficiency for sustainable NLP, and iii) evaluating and improving robustness of deep learning systems. Below are my publications.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://arxiv.org/abs/2307.08678">
                <papertitle>Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations</papertitle>
              </a>
              <br>
              <strong>Yanda Chen</strong>, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, Kathleen McKeown
              <br>
              <em>arXiv preprint</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2307.08678.pdf">paper</a>
              <p></p>
              <p>We propose to evaluate the <em>counterfactual simulatability</em> of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. We implemented two metricsâ€”precision and generality, and found that <em>i)</em> LLM's explanations have low precision, and <em>ii)</em> precision does not correlate with plausibility.</p>
            </td>
          </tr> 
		
	  <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://arxiv.org/abs/2209.07661">
                <papertitle>On the Relation between Sensitivity and Accuracy in In-context Learning</papertitle>
              </a>
              <br>
              <strong>Yanda Chen</strong>, Chen Zhao, Zhou Yu, Kathleen McKeown, He He
              <br>
              <em>arXiv preprint</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2209.07661.pdf">paper</a>
              <p></p>
              <p>We find that label bias obscures true ICL sensitivity and that ICL sensitivity is strongly and negatively correlated with accuracy. Motivated by our study, we propose <em>SenSel</em>, a few-shot selective prediction method based on ICL sensitivity.</p>
            </td>
          </tr> 
	
	  <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://arxiv.org/abs/2212.10670">
                <papertitle>In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models</papertitle>
              </a>
              <br>
              Yukun Huang, <strong>Yanda Chen</strong>, Zhou Yu, Kathleen McKeown
              <br>
              <em>arXiv preprint</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2212.10670.pdf">paper</a>
              <p></p>
              <p>We proposed in-context learning distillation, which transfers in-context learning (ICL) ability from large language models to small language models by augmenting in-context tuning with teacher-student distillation. Experiments on LAMA and CrossFit show that in-context learning distillation improves the ICL ability of small language models.</p>
            </td>
          </tr> 
		
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://aclanthology.org/2022.acl-long.53/">
                <papertitle>Meta-learning via Language Model In-context Tuning</papertitle>
              </a>
              <br>
              <strong>Yanda Chen</strong>, Ruiqi Zhong, Sheng Zha, George Karypis, He He
              <br>
              <em>ACL</em>, 2022
              <br>
              <a href="https://aclanthology.org/2022.acl-long.53.pdf">paper</a> &nbsp/&nbsp
	      <a href="https://github.com/yandachen/In-context-Tuning">code</a> &nbsp/&nbsp
	      <a href="data/ICT-ACL-Slides.pdf">slides</a> 
              <p></p>
              <p>We propose a novel few-shot meta-learning method called <em>in-context tuning</em>, where training examples are used as prefix in-context demonstrations for task adaptation. We show that in-context tuning out-performs MAML in terms of accuracy and eliminates several well-known oversensitivity artifacts of few-shot language model prompting. </p>
            </td>
          </tr> 

		
	  <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://aclanthology.org/2021.acl-long.300/">
                <papertitle>Cross-language Sentence Selection via Data Augmentation and Rationale Training</papertitle>
              </a>
              <br>
              <strong>Yanda Chen</strong>, Chris Kedzie, Suraj Nair, Petra Galuscakova, Rui Zhang, Douglas Oard, Kathleen McKeown
              <br>
              <em>ACL</em>, 2021
              <br>
              <a href="https://aclanthology.org/2021.acl-long.300.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/yandachen/Low-resource-CLSS">code</a> &nbsp/&nbsp
              <a href="https://underline.io/lecture/25655-cross-language-sentence-selection-via-data-augmentation-and-rationale-training">talk</a> &nbsp/&nbsp
              <a href="data/clss_slides.key">slides</a>
              <p></p>
              <p>We propose a data augmentation strategy and a rationale training strategy for cross-lingual sentence selection in low-resource settings where no labeled relevance judgment is available for training. Our methods achieve state-of-the-art results on three language pairs.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2010.12776">
              <papertitle>Improved Synthetic Training for Reading Comprehension
</papertitle>
              </a>
              <br>
              <strong>Yanda Chen</strong>, Md Arafat Sultan, Vittorio Castelli
              <br>
              <em>arXiv preprint</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2010.12776">paper</a>
              <p></p>
              <p>We propose two novel synthetic training strategies: targeted synthetic pre-training (a method to select useful synthetic examples to target weakness of existing models) and synthetic knowledge distillation. The two techniques, when combined, yield QA models that are simultaneously smaller, faster, and more accurate. </p>
            </td>
	  </tr>
					
					
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://aclanthology.org/D19-1483/">
                <papertitle>Detecting and Reducing Bias in a High Stakes Domain</papertitle>
              </a>
              <br>
              Ruiqi Zhong, <strong>Yanda Chen</strong>, Desmond Patton, Charlotte Selous, Kathy McKeown
              <br>
							<em>EMNLP</em>, 2019
              <br>
              <a href="https://aclanthology.org/D19-1483/">paper</a>
              /
              <a href="https://github.com/yandachen/GI_2019">code</a>
              /
              <a href="data/gi_poster.pdf">poster</a>
              <p></p>
              <p>We propose a framework to systematically detect and reduce language bias of deep learning models under the high-stakes context of gang intervention.</p>
            </td>
          </tr> 

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching Assistant</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              Natural Language Processing, Spring 2022 & Spring 2021
              <br><br>
              Analysis of Algorithms, Spring 2021 & Spring 2020
              <br><br>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website design from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
